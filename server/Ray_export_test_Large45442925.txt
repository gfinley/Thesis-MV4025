2022-09-22 15:30:40,554	INFO worker.py:1518 -- Started a local Ray instance.
2022-09-22 15:30:43,380	INFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.
2022-09-22 15:30:43,381	INFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2022-09-22 15:30:48,368	WARNING util.py:65 -- Install gputil for GPU system monitoring.
agent_timesteps_total: 4000
counters:
  num_agent_steps_sampled: 4000
  num_agent_steps_trained: 4000
  num_env_steps_sampled: 4000
  num_env_steps_trained: 4000
custom_metrics: {}
date: 2022-09-22_15-31-01
done: false
episode_len_mean: 14.904942965779467
episode_media: {}
episode_reward_max: 415.0
episode_reward_mean: 80.83650190114068
episode_reward_min: 25.0
episodes_this_iter: 263
episodes_total: 263
experiment_id: 2fadd802921c46fc9d0b8201ea60b6d6
hostname: compute-2-39.hamming.cluster
info:
  learner:
    default_policy:
      custom_metrics: {}
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0000000000000016e-05
        entropy: 1.937339626717311
        entropy_coeff: 0.0
        grad_gnorm: 0.46264293305976417
        kl: 0.008694136572223066
        policy_loss: -0.02382570164558548
        total_loss: 9.869546262679561
        vf_explained_var: 2.9008875611007856e-06
        vf_loss: 9.891633090152535
      model: {}
      num_agent_steps_trained: 128.0
  num_agent_steps_sampled: 4000
  num_agent_steps_trained: 4000
  num_env_steps_sampled: 4000
  num_env_steps_trained: 4000
iterations_since_restore: 1
node_ip: 10.1.2.39
num_agent_steps_sampled: 4000
num_agent_steps_trained: 4000
num_env_steps_sampled: 4000
num_env_steps_sampled_this_iter: 4000
num_env_steps_trained: 4000
num_env_steps_trained_this_iter: 4000
num_faulty_episodes: 0
num_healthy_workers: 20
num_recreated_workers: 0
num_steps_trained_this_iter: 4000
perf:
  cpu_util_percent: 5.1157894736842096
  ram_util_percent: 7.599999999999999
pid: 38070
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05854203297228065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.8275081817163384
  mean_inference_ms: 1.1218456842864006
  mean_raw_obs_processing_ms: 0.7123029018019855
sampler_results:
  custom_metrics: {}
  episode_len_mean: 14.904942965779467
  episode_media: {}
  episode_reward_max: 415.0
  episode_reward_mean: 80.83650190114068
  episode_reward_min: 25.0
  episodes_this_iter: 263
  hist_stats:
    episode_lengths: [20, 20, 20, 15, 20, 15, 10, 15, 20, 20, 15, 10, 10, 15, 10,
      10, 20, 10, 10, 20, 15, 20, 20, 20, 10, 15, 20, 10, 20, 20, 10, 20, 20, 20,
      10, 10, 10, 10, 20, 20, 20, 15, 10, 15, 10, 10, 15, 15, 20, 15, 15, 15, 20,
      20, 15, 15, 10, 20, 10, 10, 15, 15, 15, 20, 15, 10, 10, 10, 15, 10, 20, 20,
      10, 20, 15, 20, 20, 15, 10, 20, 20, 15, 20, 10, 20, 20, 10, 15, 10, 15, 20,
      20, 10, 10, 10, 20, 20, 20, 15, 15, 15, 15, 10, 20, 20, 10, 10, 10, 15, 15,
      20, 20, 20, 10, 15, 10, 20, 20, 10, 20, 15, 10, 10, 10, 15, 20, 15, 20, 15,
      20, 20, 10, 20, 10, 10, 10, 20, 20, 10, 20, 15, 20, 10, 10, 15, 10, 15, 20,
      15, 10, 10, 15, 20, 20, 10, 15, 10, 10, 15, 10, 10, 15, 10, 20, 20, 15, 10,
      20, 20, 20, 10, 15, 15, 15, 20, 15, 10, 15, 15, 15, 15, 15, 15, 20, 20, 15,
      15, 20, 15, 20, 10, 15, 10, 15, 20, 10, 10, 15, 15, 10, 10, 20, 20, 20, 15,
      10, 10, 10, 20, 20, 20, 10, 20, 15, 15, 20, 10, 10, 10, 15, 10, 10, 10, 20,
      15, 20, 15, 10, 15, 10, 10, 15, 10, 15, 10, 15, 15, 10, 15, 10, 15, 10, 10,
      10, 15, 15, 20, 20, 20, 10, 10, 15, 10, 15, 20, 15, 10, 15, 20, 20, 15, 15,
      10]
    episode_reward: [25.0, 217.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0,
      75.0, 25.0, 265.0, 217.0, 125.0, 169.0, 415.0, 25.0, 25.0, 25.0, 25.0, 25.0,
      265.0, 25.0, 169.0, 265.0, 25.0, 25.0, 73.0, 25.0, 25.0, 25.0, 125.0, 50.0,
      25.0, 25.0, 169.0, 25.0, 75.0, 25.0, 25.0, 25.0, 75.0, 25.0, 265.0, 25.0, 25.0,
      25.0, 121.0, 267.0, 125.0, 25.0, 75.0, 25.0, 75.0, 25.0, 265.0, 25.0, 25.0,
      25.0, 25.0, 73.0, 217.0, 265.0, 25.0, 121.0, 25.0, 217.0, 25.0, 25.0, 25.0,
      75.0, 265.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 125.0, 219.0,
      269.0, 125.0, 25.0, 75.0, 25.0, 25.0, 267.0, 25.0, 267.0, 25.0, 121.0, 75.0,
      25.0, 75.0, 217.0, 73.0, 25.0, 25.0, 265.0, 169.0, 25.0, 25.0, 25.0, 217.0,
      25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 121.0, 25.0, 25.0, 125.0, 265.0, 25.0, 25.0,
      265.0, 25.0, 75.0, 125.0, 25.0, 25.0, 125.0, 169.0, 25.0, 25.0, 25.0, 25.0,
      265.0, 25.0, 25.0, 25.0, 25.0, 25.0, 265.0, 25.0, 267.0, 75.0, 25.0, 25.0, 25.0,
      315.0, 219.0, 25.0, 25.0, 25.0, 265.0, 25.0, 25.0, 121.0, 25.0, 25.0, 175.0,
      25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 217.0, 267.0, 25.0, 25.0, 265.0,
      25.0, 25.0, 217.0, 25.0, 25.0, 265.0, 121.0, 25.0, 75.0, 265.0, 25.0, 265.0,
      25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 169.0, 265.0, 25.0, 25.0, 265.0,
      75.0, 25.0, 25.0, 25.0, 25.0, 25.0, 75.0, 25.0, 25.0, 25.0, 25.0, 73.0, 25.0,
      25.0, 25.0, 169.0, 75.0, 265.0, 75.0, 173.0, 25.0, 169.0, 75.0, 25.0, 25.0,
      73.0, 217.0, 169.0, 25.0, 217.0, 265.0, 25.0, 25.0, 25.0, 25.0, 25.0, 121.0,
      315.0, 75.0, 75.0, 25.0, 25.0, 25.0, 25.0, 25.0, 75.0, 265.0, 25.0, 25.0, 25.0,
      125.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 121.0, 25.0, 25.0, 25.0, 75.0, 25.0,
      25.0, 25.0, 25.0, 121.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05854203297228065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.8275081817163384
    mean_inference_ms: 1.1218456842864006
    mean_raw_obs_processing_ms: 0.7123029018019855
time_since_restore: 12.973201751708984
time_this_iter_s: 12.973201751708984
time_total_s: 12.973201751708984
timers:
  learn_throughput: 326.249
  learn_time_ms: 12260.582
  load_throughput: 15210531.278
  load_time_ms: 0.263
  synch_weights_time_ms: 4.053
  training_iteration_time_ms: 12966.022
timestamp: 1663885861
timesteps_since_restore: 0
timesteps_total: 4000
training_iteration: 1
trial_id: default
warmup_time: 4.989675521850586

Traceback (most recent call last):
  File "ray_run.py", line 317, in <module>
    save_SB(algo,"/home/matthew.finley/Thesis-MV4025/server/ray_models/"+run_name)
  File "ray_run.py", line 281, in save_SB
    exclude = set(exclude).union(model._excluded_save_params())
AttributeError: 'PPO' object has no attribute '_excluded_save_params'
